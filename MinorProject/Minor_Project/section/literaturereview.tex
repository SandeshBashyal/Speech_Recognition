\chapter{LITERATURE REVIEW}
Home automation has evolved significantly as it enhances convenience and efficiency in household tasks. There are various technologies to achieve home automation such as sensor based automation, Hub based systems, machine learning, cloud based automation, etc.There have been several research projects intended to automate various tasks. In recent times there has been an increase in the use of artificial intelligent robots, neural networks, Natural language understanding and generation,voice recognition and so on in various fields \cite{adekola2019voice}. Thus, there has been an increase in the use of technology to manage household devices. There have been commercial and research projects on smart homes and speech recognition systems through use of protocols like Zigbee \cite{alshu2011voice}, Z-Wave, or Wi-Fi protocols. To build a reliable, compact, fast and low cost home automation system bluetooth communication protocol can be used \cite{das2016bluetooth}. So, home automation has been successful through use of speech recognition \cite{adekola2019voice}\cite{alshu2011voice} and still evolving till the date \cite{li2022recent}.

To do any ML task, the first and foremost need is access to the dataset and thatâ€™s the case for the speech recognition too. Speech recognition has always been a state-of-the-art task in the field of ML and AI. Speech recognition is the recognition of what has been said by the speaker to understand the intent of the speaker \cite{adekola2019voice}. However, its implementation on the edge devices like Raspberry Pi is challenging but has been made possible as in \cite{gondi2021performance}. Speech recognition is done by wake-word detection algorithm and the speech-to-text model. We have sequence-to-sequence networks like RNN, LSTM, and self-attention based models like transformers to perform ASR \cite{benzeghiba2007automatic}.  

 Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, was firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures \cite{vaswani2017attention}. Recurrent Neural Networks (RNNs) and their variant LSTMs, while powerful, suffer from limitations. LSTMs attempt to address the vanishing gradient problem in RNNs, where crucial information from earlier parts of the sequence gets lost. However, LSTMs can still struggle with very long sequences. Additionally, both RNNs and LSTMs process information sequentially, limiting their ability to capture complex relationships within the data. This sequential nature also makes them computationally expensive for training. These drawbacks hinder their performance in tasks requiring long-range dependencies and efficient processing, paving the way for advancements like transformers.\\
The drawbacks of these models being inability to generalize to long sequences, unparalizability are solved by Transformer. Transformers\cite{vaswani2017attention} outperform LSTMs in sequence modeling tasks like machine translation due to their powerful attention mechanism. Unlike LSTMs, which process information sequentially, transformers can attend to all parts of the input sequence simultaneously using self-attention. This allows them to capture long-range dependencies more effectively, crucial for understanding complex relationships in language. Additionally, encoders and decoders within transformers are specifically designed to handle input and output sequences, respectively, leading to better focus and efficiency compared to LSTMs' single, recurrent processing. This combination of self-attention and dedicated encoder-decoder architecture grants transformers superior accuracy in various NLP tasks. \\

For Automatic speech recognition, new speech recognition model using attention-based recurrent networks was used \cite{chorowski2015attention}. Unlike traditional methods, the model can focus on important parts of the speech throughout the sequence. This is useful for long and noisy speech recognition tasks. The model was tested  on a phoneme recognition task and show that it performs competitively, especially for longer speech samples.\cite{chorowski2015attention}. \newline

The Transformer model was first introduced in Gaussian mixture hidden Markov modeling for speech recognition\cite{1198704}, in order to reduce sequential calculations and the number of operations for correlating input and output position signals\cite{orken2022study}.The model performed exceptionally well in compared to state-of-the-art models at the time.

Speech recognition relies on labeled data, limiting its reach. Baevski \cite{baevski2021unsupervised} proposes wav2vec-U, an unsupervised method using self-supervised learning from wav2vec 2.0 representations. k-means clustering segments speech, and adversarial training maps segments to phonemes, even allowing for silence labels. wav2vec-U achieves significant reductions in phone error rates on TIMIT and rivals supervised models on Librispeech, demonstrating its effectiveness across languages. This approach paves the way for speech recognition in low-resource settings. Experiments on the standard Librispeech benchmark show performance close to the state of the art models from only a few years ago, even though these models relied on nearly 1,000 hours of labeled data\cite{baevski2021unsupervised}. \newline

The model, Whisper, leverages a massive dataset of audio recordings with corresponding, but imperfect, internet transcripts. This weak supervision approach, alongside multilingual and multitask training, allows Whisper to excel in zero-shot generalization and achieve competitive performance compared to fully supervised models. Furthermore, Whisper simplifies the recognition pipeline by eliminating the text normalization step. These findings highlight the potential of weak supervision for robust and efficient speech recognition.The smallest zero-shot Whisper model, which has only 39 million parameters and a 6.7 WER on LibriSpeech test-clean is roughly competitive with the best supervised LibriSpeech model when evaluated on other datasets \cite{whisper}.Whisper showed high transcription performances (for each language the percentage of correct words transcribed is higher than 90\%). For the English and Italian languages, the most committed mistakes were words substitutions, while for Russian there were observed higher percentages of both words substitution and insertion errors \cite{amorese2023automatic}. \newline.