\chapter{LITERATURE REVIEW}

Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, was firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures \cite{vaswani2017attention}.Recurrent Neural Networks (RNNs) and their variant LSTMs, while powerful, suffer from limitations. LSTMs attempt to address the vanishing gradient problem in RNNs, where crucial information from earlier parts of the sequence gets lost. However, LSTMs can still struggle with very long sequences. Additionally, both RNNs and LSTMs process information sequentially, limiting their ability to capture complex relationships within the data. This sequential nature also makes them computationally expensive for training. These drawbacks hinder their performance in tasks requiring long-range dependencies and efficient processing, paving the way for advancements like transformers.
\newline The drawbacks of these models being inability to generalize to long sequences, unparalizability are solved by Transformer.Transformers\cite{vaswani2017attention} outperform LSTMs in sequence modeling tasks like machine translation due to their powerful attention mechanism. Unlike LSTMs, which process information sequentially, transformers can attend to all parts of the input sequence simultaneously using self-attention. This allows them to capture long-range dependencies more effectively, crucial for understanding complex relationships in language. Additionally, encoders and decoders within transformers are specifically designed to handle input and output sequences, respectively, leading to better focus and efficiency compared to LSTMs' single, recurrent processing. This combination of self-attention and dedicated encoder-decoder architecture grants transformers superior accuracy in various NLP tasks. \\

For Automatic speech recognition, new speech recognition model using attention-based recurrent networks was used \cite{effectivenesschorowski2015attention}. Unlike traditional methods, the model can focus on important parts of the speech throughout the sequence. This is useful for long and noisy speech recognition tasks. The model was tested  on a phoneme recognition task and show that it performs competitively, especially for longer speech samples.\cite{effectivenesschorowski2015attention}. \newline

The Transformer model was first introduced in Gaussian mixture hidden Markov modeling for speech recognition\cite{1198704}, in order to reduce sequential calculations and the number of operations for correlating input and output position signals\cite{orken2022study}.The model performed exceptionally well in compared to state-of-the-art models at the time.

Speech recognition relies on labeled data, limiting its reach. Baevski \cite{baevski2021unsupervised} proposes wav2vec-U, an unsupervised method using self-supervised learning from wav2vec 2.0 representations. k-means clustering segments speech, and adversarial training maps segments to phonemes, even allowing for silence labels. wav2vec-U achieves significant reductions in phone error rates on TIMIT and rivals supervised models on Librispeech, demonstrating its effectiveness across languages. This approach paves the way for speech recognition in low-resource settings. Experiments on the standard Librispeech benchmark show performance close to the state of the art models from only a few years ago, even though these models relied on nearly 1,000 hours of labeled data\cite{baevski2021unsupervised}. \newline

The model, Whisper, leverages a massive dataset of audio recordings with corresponding, but imperfect, internet transcripts. This weak supervision approach, alongside multilingual and multitask training, allows Whisper to excel in zero-shot generalization and achieve competitive performance compared to fully supervised models. Furthermore, Whisper simplifies the recognition pipeline by eliminating the text normalization step. These findings highlight the potential of weak supervision for robust and efficient speech recognition.The smallest zero-shot Whisper model, which has only 39 million parameters and a 6.7 WER on LibriSpeech test-clean is roughly competitive with the best supervised LibriSpeech model when evaluated on other datasets \cite{whisper}.Whisper showed high transcription performances (for each language the percentage of correct words transcribed is higher than 90\%). For the English and Italian languages, the most committed mistakes were words substitutions, while for Russian there were observed higher percentages of both words substitution and insertion errors \cite{amorese2023automatic}. \newline